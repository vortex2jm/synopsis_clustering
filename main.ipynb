{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciência de Dados - Trabalho Prático\n",
    "\n",
    "> **Nomes:** Bruno Santos Fernandes, João Paulo Moura Clevelares, Thamya Vieira Hashimoto Donadia <br>\n",
    "> **Matrículas:** 2021100784, 2021100149, 2021100146 <br>\n",
    "> **E-mails:** {bruno.s.fernandes, joao.clevelares, thamya.donadia}@edu.ufes.br <br>\n",
    "> **Curso:** Engenharia de Computação <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação de bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos dados textuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregamento do dataset \n",
    "df = pd.read_csv(\"./filmes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenção das informações gerais do dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando as features do dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a feature a ser processada (sinopse)\n",
    "df['sinopse'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisão do texto em sentenças e palavras\n",
    "df['sentences'] = df['sinopse'].apply(sent_tokenize)\n",
    "df['tokens'] = df['sinopse'].apply(word_tokenize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversão do texto para letras minúsculas\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token.lower() for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de símbolos de pontuação de cada token\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token.translate(table) for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversão de caracteres especiais\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [unidecode.unidecode(token) for token in x])\n",
    "df['tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de tokens que não são palavras\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token.isalpha()])\n",
    "df['tokens'].head(10)\n",
    "\n",
    "# TODO: Talvez seja necessário usar alguns tokens númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "df['tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming \n",
    "df['tokens'] = df['tokens'].apply(lambda x: [PorterStemmer().stem(token) for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[[\"sinopse\", \"tokens\", \"genres\"]].sample(frac=0.3, random_state=42)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construção da matriz de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando a matriz de contagem de termos \n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(sample[\"sinopse\"])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando a frequência de documentos em que cada termo aparece\n",
    "doc_freq = np.array((X_counts > 0).sum(axis=0)).flatten()\n",
    "df_vocab = pd.DataFrame({'termo': vocab, 'doc_freq': doc_freq})\n",
    "df_vocab[df_vocab['doc_freq'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# análise estatística descritiva\n",
    "mean = np.mean(doc_freq)\n",
    "median = np.median(doc_freq)\n",
    "percentiles = np.percentile(doc_freq, [25, 50, 75])\n",
    "\n",
    "print(\"Estatísticas da frequência dos termos:\")\n",
    "print(f\"Média: {mean:.2f}\")\n",
    "print(f\"Mediana: {median}\")\n",
    "print(f\"Percentis 25, 50 e 75: {percentiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotando o histrogama da frequência dos termos\n",
    "sns.displot(df_vocab, x=df_vocab['doc_freq'], kde=True, bins=50, log_scale=(True, False))\n",
    "plt.ylabel('Número de termos')\n",
    "plt.xlabel('Número de Documentos em que o termo aparece')\n",
    "plt.title('Distribuição da Frequência dos Termos no Corpus')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_vocab['doc_freq'])\n",
    "plt.xlabel('Número de Documentos em que o termo aparece')\n",
    "plt.title('Boxplot da Frequência dos Termos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(sample[\"tokens\"].apply(lambda tokens: \" \".join(tokens)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = pd.DataFrame(X.todense(), columns = vectorizer.get_feature_names_out())\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de dimensionalidade, via Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_full = X.shape[1]\n",
    "svd_full = TruncatedSVD(n_components=n_components_full)\n",
    "svd_full.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotando a variância cumulativa\n",
    "cumulative_variance = np.cumsum(svd_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(np.arange(1, min(X.shape[0], X.shape[1]) + 1), cumulative_variance)\n",
    "plt.xlabel(r'$k$ - Número de componentes principais')\n",
    "plt.ylabel(r'$f(k)$ - Fração cumulativa da variância explicada')\n",
    "plt.title('Variância Explicada Cumulativa com TruncatedSVD')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_components = 3000\n",
    "svd = TruncatedSVD(n_components=new_n_components)\n",
    "X2 = svd.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "normalizer = Normalizer(norm='l2')\n",
    "X_normalized = normalizer.fit_transform(X2.copy())\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inércia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inércial\n",
    "inertia = []\n",
    "for i in range(1, 30):\n",
    "  km = KMeans(n_clusters = i)\n",
    "  km.fit(X_normalized.copy())\n",
    "  inertia.append(km.inertia_)\n",
    "\n",
    "# Scatter\n",
    "plt.scatter(range(1, 30), inertia)\n",
    "_ = plt.ylabel(\"Função Objetivo\")\n",
    "_ = plt.xlabel(r\"$k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhueta V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = X_normalized.copy()\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "range_n_clusters = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 0.01])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(Xs) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(Xs)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(Xs, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(Xs, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhueta V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def sc_evaluate_clusters(X, max_clusters, n_init, seed):\n",
    "    s = np.zeros(max_clusters+1)\n",
    "    s[0] = 0\n",
    "    s[1] = 0\n",
    "    for k in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = n_init, random_state = seed)\n",
    "        kmeans.fit_predict(X)\n",
    "        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric = 'euclidean')\n",
    "    return s\n",
    "\n",
    "s = sc_evaluate_clusters(X_normalized.copy(), 20, 10, 1)\n",
    "plt.plot(range(2, len(s)), s[2:], 'o-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.title('$k$-means clustering performance on synthetic data')\n",
    "plt.ylabel('Silhouette Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 7\n",
    "\n",
    "# Clusterização\n",
    "kmeans = KMeans(n_clusters = K)\n",
    "kmeans.fit(X_normalized.copy())\n",
    "\n",
    "\n",
    "y_kmeans = kmeans.predict(X_normalized.copy())  # neste caso ele esta reprevendo os rotulos dos dados de treinamento, pois essa função é pra predizer novos dados\n",
    "# ou y_means = kmeans.labels_ # Pega os rotulos dos dados de treinamento\n",
    "\n",
    "# Vetor com os clusters de cada sinopse\n",
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduzindo dimensionalidade para o plot\n",
    "pca = PCA(n_components=3)\n",
    "X2_reduced = pca.fit_transform(X_normalized.copy())\n",
    "X2_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando  clusters em 3D\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X2_reduced[:, 0], X2_reduced[:, 1], X2_reduced[:, 2], c=y_kmeans, cmap=plt.cm.tab20, s=5)\n",
    "\n",
    "# Rótulos dos eixos\n",
    "ax.set_xlabel('Componente 1')\n",
    "ax.set_ylabel('Componente 2')\n",
    "ax.set_zlabel('Componente 3')\n",
    "\n",
    "# plt.xlabel('Componente 1')\n",
    "# plt.ylabel('Componente 2')\n",
    "# plt.title('Visualização dos Clusters com KMeans')\n",
    "# plt.colorbar(label='Cluster', ticks=range(20))\n",
    "# plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentual de Gêneros em cada Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando nova coluna com os clusters\n",
    "sample_kmeans = sample.copy()\n",
    "sample_kmeans[\"cluster\"] = y_kmeans\n",
    "sample_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando Generos agrupados nas linhas\n",
    "sample_kmeans[\"genres\"] = sample_kmeans[\"genres\"].str.split(',')\n",
    "sample_kmeans = sample_kmeans.explode(\"genres\")\n",
    "sample_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregando por (cluster + genero) para descobrir frequencia\n",
    "genre_frequency_kmeans = sample_kmeans.groupby([\"genres\", \"cluster\"]).size().reset_index(name=\"freq\")\n",
    "genre_frequency_kmeans\n",
    "\n",
    "# Transformando amostra em formato matricial\n",
    "df_pivot_kmeans = genre_frequency_kmeans.pivot_table(index='genres', columns='cluster', values='freq', fill_value=0)\n",
    "df_pivot_kmeans = df_pivot_kmeans.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_pivot_kmeans, cmap=\"Reds\", linewidths=1)\n",
    "plt.title('Heatmap de Frequência de Gêneros por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Gênero')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigengap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse import csgraph\n",
    "from numpy import linalg as LA\n",
    "\n",
    "# 1 Construindo a matriz de adjacências do grafo de vizinhos mais próximos.\n",
    "G = kneighbors_graph(X_normalized.copy(), n_neighbors = 300, include_self = True)\n",
    "A = 0.5 * (G + G.T)\n",
    "\n",
    "# 2 Construindo a Laplaciana Normalizada\n",
    "L = csgraph.laplacian(A, normed = True).todense()\n",
    "\n",
    "# 3 Obtendo os autovalores da Laplaciana Normalizada\n",
    "# Valores já estão ordenados em ordem crescente.\n",
    "values, _ = LA.eigh(L)\n",
    "\n",
    "# 4 Plotando os valores dos 'gaps' e escolhendo um k adequado.\n",
    "plt.scatter([i for i in range(1, 21)], values[:20])\n",
    "plt.xlabel('Índice do autovalor')\n",
    "plt.ylabel('Autovalor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamento Espectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "spectral = SpectralClustering(n_clusters=K, assign_labels='discretize', random_state=0).fit(X_normalized.copy())\n",
    "spectral.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_spectral = sample.copy()\n",
    "sample_spectral[\"cluster\"] = spectral.labels_\n",
    "sample_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando Generos agrupados nas linhas\n",
    "sample_spectral[\"genres\"] = sample_spectral[\"genres\"].str.split(',')\n",
    "sample_spectral = sample_spectral.explode(\"genres\")\n",
    "sample_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_frequency_spectral = sample_spectral.groupby([\"genres\", \"cluster\"]).size().reset_index(name=\"freq\")\n",
    "genre_frequency_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_spectral = genre_frequency_spectral.pivot_table(index='genres', columns='cluster', values='freq', fill_value=0)\n",
    "df_pivot_spectral = df_pivot_spectral.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_pivot_spectral, cmap=\"Reds\", linewidths=1)\n",
    "plt.title('Heatmap de Frequência de Gêneros por Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Gênero')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
