{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciência de Dados - Trabalho Prático\n",
    "\n",
    "> **Nomes:** Bruno Santos Fernandes, João Paulo Moura Clevelares, Thamya Vieira Hashimoto Donadia <br>\n",
    "> **Matrículas:** 2021100784, 2021100149, 2021100146 <br>\n",
    "> **E-mails:** {bruno.s.fernandes, joao.clevelares, thamya.donadia}@edu.ufes.br <br>\n",
    "> **Curso:** Engenharia de Computação <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação de bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-processamento dos dados textuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregamento do dataset \n",
    "df = pd.read_csv(\"./filmes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenção das informações gerais do dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando as features do dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a feature a ser processada (sinopse)\n",
    "df['sinopse'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisão do texto em sentenças e palavras\n",
    "df['sentences'] = df['sinopse'].apply(sent_tokenize)\n",
    "df['tokens'] = df['sinopse'].apply(word_tokenize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversão do texto para letras minúsculas\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token.lower() for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de símbolos de pontuação de cada token\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token.translate(table) for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversão de caracteres especiais\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [unidecode.unidecode(token) for token in x])\n",
    "df['tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de tokens que não são palavras\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token.isalpha()])\n",
    "df['tokens'].head(10)\n",
    "\n",
    "# TODO: Talvez seja necessário usar alguns tokens númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [token for token in x if token not in stop_words])\n",
    "df['tokens'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming \n",
    "df['tokens'] = df['tokens'].apply(lambda x: [PorterStemmer().stem(token) for token in x])\n",
    "df['tokens'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construção da matriz de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerando a matriz de contagem de termos \n",
    "vectorizer = CountVectorizer()\n",
    "X_counts = vectorizer.fit_transform(df['sinopse'])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculando a frequência de documentos em que cada termo aparece\n",
    "doc_freq = np.array((X_counts > 0).sum(axis=0)).flatten()\n",
    "df_vocab = pd.DataFrame({'termo': vocab, 'doc_freq': doc_freq})\n",
    "df_vocab[df_vocab['doc_freq'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# análise estatística descritiva\n",
    "mean = np.mean(doc_freq)\n",
    "median = np.median(doc_freq)\n",
    "percentiles = np.percentile(doc_freq, [25, 50, 75])\n",
    "\n",
    "print(\"Estatísticas da frequência dos termos:\")\n",
    "print(f\"Média: {mean:.2f}\")\n",
    "print(f\"Mediana: {median}\")\n",
    "print(f\"Percentis 25, 50 e 75: {percentiles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotando o histrogama da frequência dos termos\n",
    "sns.displot(df_vocab, x=df_vocab['doc_freq'], kde=True, bins=50, log_scale=(True, False))\n",
    "plt.ylabel('Número de termos')\n",
    "plt.xlabel('Número de Documentos em que o termo aparece')\n",
    "plt.title('Distribuição da Frequência dos Termos no Corpus')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df_vocab['doc_freq'])\n",
    "plt.xlabel('Número de Documentos em que o termo aparece')\n",
    "plt.title('Boxplot da Frequência dos Termos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(df['tokens'].apply(lambda tokens: \" \".join(tokens)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = pd.DataFrame(X.todense(), columns = vectorizer.get_feature_names_out())\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redução de dimensionalidade, via Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_full = X.shape[1]\n",
    "svd_full = TruncatedSVD(n_components=n_components_full)\n",
    "svd_full.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotando a variância cumulativa\n",
    "cumulative_variance = np.cumsum(svd_full.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(np.arange(1, n_components_full + 1), cumulative_variance)\n",
    "plt.xlabel(r'$k$ - Número de componentes principais')\n",
    "plt.ylabel(r'$f(k)$ - Fração cumulativa da variância explicada')\n",
    "plt.title('Variância Explicada Cumulativa com TruncatedSVD')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_components = 6000\n",
    "svd = TruncatedSVD(n_components=new_n_components)\n",
    "X2 = svd.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
